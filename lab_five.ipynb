{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 5: translation simple ecoder-decocer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from contra_qa.plots.functions  import simple_step_plot\n",
    "import  matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/boolean2_train.csv\")\n",
    "\n",
    "df2train = df2.iloc[:8500]\n",
    "df2valid = df2.iloc[8500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>and_A</th>\n",
       "      <th>and_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>Agnes is proud and Lauren is ambitious</td>\n",
       "      <td>Agnes is not proud</td>\n",
       "      <td>Agnes is proud</td>\n",
       "      <td>Lauren is ambitious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>Curtis is wonderful and Jessie is ambitious</td>\n",
       "      <td>Curtis is not wonderful</td>\n",
       "      <td>Curtis is wonderful</td>\n",
       "      <td>Jessie is ambitious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>Brett is dead and Tracy is important</td>\n",
       "      <td>Tracy is not important</td>\n",
       "      <td>Brett is dead</td>\n",
       "      <td>Tracy is important</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>Lauren is hallowed and Yvette is shy</td>\n",
       "      <td>Yvette is not shy</td>\n",
       "      <td>Lauren is hallowed</td>\n",
       "      <td>Yvette is shy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>Kathleen is alive and Dustin is clever</td>\n",
       "      <td>Dustin is not clever</td>\n",
       "      <td>Kathleen is alive</td>\n",
       "      <td>Dustin is clever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentence1                 sentence2  \\\n",
       "8495       Agnes is proud and Lauren is ambitious        Agnes is not proud   \n",
       "8496  Curtis is wonderful and Jessie is ambitious   Curtis is not wonderful   \n",
       "8497         Brett is dead and Tracy is important    Tracy is not important   \n",
       "8498         Lauren is hallowed and Yvette is shy         Yvette is not shy   \n",
       "8499       Kathleen is alive and Dustin is clever      Dustin is not clever   \n",
       "\n",
       "                    and_A                and_B  label  \n",
       "8495       Agnes is proud  Lauren is ambitious      1  \n",
       "8496  Curtis is wonderful  Jessie is ambitious      1  \n",
       "8497        Brett is dead   Tracy is important      1  \n",
       "8498   Lauren is hallowed        Yvette is shy      1  \n",
       "8499    Kathleen is alive     Dustin is clever      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\n",
      "\n",
      "After: ddddda capoeeeeeee ! ! aas fdf\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "example = \"ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\"\n",
    "print(\"Before:\", example)\n",
    "print()\n",
    "print(\"After:\", normalizeString(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_A = list(zip(list(df2train.sentence1.values), list(df2train.and_A.values)))\n",
    "pairs_B = list(zip(list(df2train.sentence1.values), list(df2train.and_B.values)))\n",
    "pairs_A = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A]\n",
    "pairs_B = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, pairs, reverse=False):\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [tuple(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_enc, eng_dec, pairs_A = readLangs(\"eng_enc\",\n",
    "                                      \"eng_dec\",\n",
    "                                      pairs_A,\n",
    "                                      reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    cond1 = len(p[0].split(' ')) < MAX_LENGTH\n",
    "    cond2 = len(p[1].split(' ')) < MAX_LENGTH \n",
    "    return cond1 and cond2\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 705\n",
      "eng_dec 704\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, pairs, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, pairs, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng_enc\",\n",
    "                                             \"eng_dec\",\n",
    "                                             pairs_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alfred is helpful and annette is important', 'alfred is helpful')\n"
     ]
    }
   ],
   "source": [
    "example = random.choice(pairs)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[114, 3, 83, 5, 508, 3, 100]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFromSentence(input_lang,example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 3, 89]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFromSentence(output_lang, example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[114],\n",
      "        [  3],\n",
      "        [ 83],\n",
      "        [  5],\n",
      "        [508],\n",
      "        [  3],\n",
      "        [100],\n",
      "        [  1]])\n",
      "\n",
      "torch.Size([8, 1])\n",
      "torch.int64\n",
      "tensor([[67],\n",
      "        [ 3],\n",
      "        [89],\n",
      "        [ 1]])\n",
      "\n",
      "torch.Size([4, 1])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_sen = tensorFromSentence(input_lang,example[0])\n",
    "output_sen = tensorFromSentence(output_lang, example[1])\n",
    "\n",
    "print(input_sen)\n",
    "print()\n",
    "print(input_sen.shape)\n",
    "print(input_sen.dtype)\n",
    "print(output_sen)\n",
    "print()\n",
    "print(output_sen.shape)\n",
    "print(output_sen.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "\n",
      "tensor([[114],\n",
      "        [  3],\n",
      "        [ 83],\n",
      "        [  5],\n",
      "        [508],\n",
      "        [  3],\n",
      "        [100],\n",
      "        [  1]])\n",
      "\n",
      "torch.Size([8, 1])\n",
      "torch.int64\n",
      "\n",
      "output\n",
      "\n",
      "tensor([[67],\n",
      "        [ 3],\n",
      "        [89],\n",
      "        [ 1]])\n",
      "\n",
      "torch.Size([4, 1])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_sen, output_sen = tensorsFromPair(example)\n",
    "\n",
    "\n",
    "print(\"input\\n\")\n",
    "print(input_sen)\n",
    "print()\n",
    "print(input_sen.shape)\n",
    "print(input_sen.dtype)\n",
    "\n",
    "print(\"\\noutput\\n\")\n",
    "print(output_sen)\n",
    "print()\n",
    "print(output_sen.shape)\n",
    "print(output_sen.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "eng_enc_v_size = 705\n",
    "eng_dec_v_size = 704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sen: torch.Size([8, 1]) torch.int64\n",
      "h0: torch.Size([1, 1, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "h0 = encoder.initHidden()\n",
    "print(\"input_sen:\", input_sen.shape, input_sen.dtype)\n",
    "print(\"h0:\", h0.shape, h0.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([1, 1, 10]) torch.float32\n",
      "hidden_enc: torch.Size([1, 1, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "encoder_outputs = torch.zeros(max_length,\n",
    "                              encoder.hidden_size,\n",
    "                              device=device)\n",
    "\n",
    "input_length = input_sen.size(0)\n",
    "\n",
    "for ei in range(input_length):\n",
    "    output, hidden_enc = encoder(input_sen[ei], h0)\n",
    "    h0 = hidden_enc\n",
    "    encoder_outputs[ei] = output[0, 0]\n",
    "\n",
    "print(\"output:\", output.shape, output.dtype)\n",
    "print(\"hidden_enc:\", hidden_enc.shape, hidden_enc.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output: torch.Size([1, 704]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 704]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 704]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 704]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "decoder_hidden = hidden_enc\n",
    "\n",
    "target_length = output_sen.size(0)\n",
    "\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    decoder_input = output_sen[di]  # Teacher forcing\n",
    "    print(\"decoder_output:\", decoder_output.shape, decoder_output.dtype)\n",
    "    print()\n",
    "    print(\"decoder_hidden:\", decoder_hidden.shape, decoder_hidden.dtype)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss over each token of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def train(input_tensor,\n",
    "          target_tensor,\n",
    "          encoder,\n",
    "          decoder,\n",
    "          encoder_optimizer,\n",
    "          decoder_optimizer,\n",
    "          criterion,\n",
    "          max_length=MAX_LENGTH,\n",
    "          teacher_forcing_ratio=0.5):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length,\n",
    "                                  encoder.hidden_size,\n",
    "                                  device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True\n",
    "\n",
    "    if not random.random() < teacher_forcing_ratio:\n",
    "        use_teacher_forcing = False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.topk(1)\n",
    "            decoder_input = topone.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(500):\n",
    "    loss = train(input_sen,\n",
    "          output_sen,\n",
    "          encoder,\n",
    "          decoder,\n",
    "          encoder_optimizer,\n",
    "          decoder_optimizer,\n",
    "          criterion,\n",
    "          max_length=MAX_LENGTH)\n",
    "    losses.append(loss)\n",
    "\n",
    "simple_step_plot([losses],\n",
    "                 \"loss\",\n",
    "                 \"loss example (one pair of sentence only)\",\n",
    "                 \"loss_example.png\",\n",
    "                  figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s' % asMinutes(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 1000\n",
    "# training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                   for i in range(n_iters)]\n",
    "\n",
    "training_pairs = pairs\n",
    "\n",
    "# losses = []\n",
    "# start = time.time()\n",
    "\n",
    "for t in training_pairs:\n",
    "    input_sen, output_sen = t    \n",
    "    loss = train(input_sen,\n",
    "          output_ sen,\n",
    "#           encoder,\n",
    "#           decoder,\n",
    "#           encoder_optimizer,\n",
    "#           decoder_optimizer,\n",
    "#           criterion,\n",
    "#           max_length=MAX_LENGTH)\n",
    "#     losses.append(loss)\n",
    "\n",
    "# print(timeSince(start))\n",
    "\n",
    "# simple_step_plot([losses],\n",
    "#                  \"loss\",\n",
    "#                  \"loss example ({} pair of sentences only)\".format(n_iters),\n",
    "#                  \"loss_example.png\",\n",
    "#                   figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder,\n",
    "               decoder,\n",
    "               n_iters,\n",
    "               status_every=100,\n",
    "               learning_rate=0.01,\n",
    "               teacher_forcing_ratio=0.5):\n",
    "\n",
    "    plot_losses = []\n",
    "    old = 0\n",
    "    start = time.time()\n",
    "    all_loss = []\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "    for i, t in enumerate(training_pairs):\n",
    "        input_sen, output_sen = t\n",
    "        loss = train(input_sen,\n",
    "                     output_sen,\n",
    "                     encoder,\n",
    "                     decoder,\n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer,\n",
    "                     criterion,\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        plot_losses.append(loss)\n",
    "\n",
    "        if i % status_every == 0 and i != 0:\n",
    "            print(\"mean loss = {:.2f}\\n\".format(np.mean(plot_losses)))\n",
    "            print(\"time in {} steps:\".format(status_every), timeSince(start))\n",
    "            simple_step_plot([plot_losses],\n",
    "                             \"loss\",\n",
    "                             \"loss plot (from {} to {})\".format(old, i),\n",
    "                             \"loss_example.png\",\n",
    "                             figsize=(10, 3))\n",
    "            all_loss += plot_losses\n",
    "            plot_losses = []\n",
    "            old = i\n",
    "            start = time.time()\n",
    "    \n",
    "    simple_step_plot([all_loss],\n",
    "                     \"loss\",\n",
    "                     \"loss over training\" ,\n",
    "                     \"loss_example.png\",\n",
    "                     figsize=(15, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder,\n",
    "              decoder,\n",
    "              sentence,\n",
    "              max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translation of a non trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
