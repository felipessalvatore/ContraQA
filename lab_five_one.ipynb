{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 5: translation simple ecoder-decocer over the b2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from contra_qa.plots.functions  import simple_step_plot\n",
    "import  matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/boolean2_train.csv\")\n",
    "\n",
    "df2train = df2.iloc[:8500]\n",
    "df2valid = df2.iloc[8500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>and_A</th>\n",
       "      <th>and_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>Agnes is proud and Lauren is ambitious</td>\n",
       "      <td>Agnes is not proud</td>\n",
       "      <td>Agnes is proud</td>\n",
       "      <td>Lauren is ambitious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>Curtis is wonderful and Jessie is ambitious</td>\n",
       "      <td>Curtis is not wonderful</td>\n",
       "      <td>Curtis is wonderful</td>\n",
       "      <td>Jessie is ambitious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>Brett is dead and Tracy is important</td>\n",
       "      <td>Tracy is not important</td>\n",
       "      <td>Brett is dead</td>\n",
       "      <td>Tracy is important</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>Lauren is hallowed and Yvette is shy</td>\n",
       "      <td>Yvette is not shy</td>\n",
       "      <td>Lauren is hallowed</td>\n",
       "      <td>Yvette is shy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>Kathleen is alive and Dustin is clever</td>\n",
       "      <td>Dustin is not clever</td>\n",
       "      <td>Kathleen is alive</td>\n",
       "      <td>Dustin is clever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentence1                 sentence2  \\\n",
       "8495       Agnes is proud and Lauren is ambitious        Agnes is not proud   \n",
       "8496  Curtis is wonderful and Jessie is ambitious   Curtis is not wonderful   \n",
       "8497         Brett is dead and Tracy is important    Tracy is not important   \n",
       "8498         Lauren is hallowed and Yvette is shy         Yvette is not shy   \n",
       "8499       Kathleen is alive and Dustin is clever      Dustin is not clever   \n",
       "\n",
       "                    and_A                and_B  label  \n",
       "8495       Agnes is proud  Lauren is ambitious      1  \n",
       "8496  Curtis is wonderful  Jessie is ambitious      1  \n",
       "8497        Brett is dead   Tracy is important      1  \n",
       "8498   Lauren is hallowed        Yvette is shy      1  \n",
       "8499    Kathleen is alive     Dustin is clever      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\n",
      "\n",
      "After: ddddda capoeeeeeee ! ! aas fdf\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "example = \"ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\"\n",
    "print(\"Before:\", example)\n",
    "print()\n",
    "print(\"After:\", normalizeString(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_A = list(zip(list(df2train.sentence1.values), list(df2train.and_A.values)))\n",
    "pairs_B = list(zip(list(df2train.sentence1.values), list(df2train.and_B.values)))\n",
    "pairs_A = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A]\n",
    "pairs_B = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B]\n",
    "pairs_A_val = list(zip(list(df2valid.sentence1.values), list(df2valid.and_A.values)))\n",
    "pairs_B_val = list(zip(list(df2valid.sentence1.values), list(df2valid.and_B.values)))\n",
    "pairs_A_val = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A_val]\n",
    "pairs_B_val = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, pairs, reverse=False):\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [tuple(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    cond1 = len(p[0].split(' ')) < MAX_LENGTH\n",
    "    cond2 = len(p[1].split(' ')) < MAX_LENGTH \n",
    "    return cond1 and cond2\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, pairs, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, pairs, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 705\n",
      "eng_dec 704\n",
      "\n",
      "Read 1500 sentence pairs\n",
      "Trimmed to 1500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 696\n",
      "eng_dec 644\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, training_pairs_A = prepareData(\"eng_enc\",\n",
    "                                             \"eng_dec\",\n",
    "                                             pairs_A)\n",
    "\n",
    "print()\n",
    "_, _, valid_pairs_A = prepareData(\"eng_enc\",\n",
    "                                \"eng_dec\",\n",
    "                                pairs_A_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 705\n",
      "eng_dec 704\n",
      "\n",
      "Read 1500 sentence pairs\n",
      "Trimmed to 1500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 696\n",
      "eng_dec 638\n"
     ]
    }
   ],
   "source": [
    "_, _, training_pairs_B = prepareData(\"eng_enc\",\n",
    "                                     \"eng_dec\",\n",
    "                                     pairs_B)\n",
    "print()\n",
    "_, _, valid_pairs_B = prepareData(\"eng_enc\",\n",
    "                                \"eng_dec\",\n",
    "                                pairs_B_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "eng_enc_v_size = input_lang.n_words\n",
    "eng_dec_v_size = output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderA = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoderA = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "encoderA.load_state_dict(torch.load(\"encoder4.pkl\"))\n",
    "decoderA.load_state_dict(torch.load(\"decoder4.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderB = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoderB = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "encoderB.load_state_dict(torch.load(\"encoder5.pkl\"))\n",
    "decoderB.load_state_dict(torch.load(\"decoder5.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder,\n",
    "              decoder,\n",
    "              sentence,\n",
    "              max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translation of a trained model: and A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence : penny is thankful and naomi is alive\n",
      "neural translation : penny is thankful <EOS>\n",
      "reference translation : penny is thankful <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : carlos is kind and paula is uninterested\n",
      "neural translation : carlos is kind <EOS>\n",
      "reference translation : carlos is kind <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : jack is hallowed and kent is easy\n",
      "neural translation : jack is hallowed <EOS>\n",
      "reference translation : jack is hallowed <EOS>\n",
      "blue score = 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in training_pairs_A[0:3]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = translate(encoderA,\n",
    "                                   decoderA,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(\"neural translation : \" + neural_translation)\n",
    "    reference = t[1] + ' <EOS>'\n",
    "    print(\"reference translation : \" + reference)\n",
    "    reference = reference.split(\" \")\n",
    "    candidate = neural_translation.split(\" \")\n",
    "    score = sentence_bleu([reference], candidate)\n",
    "    print(\"blue score = {:.2f}\".format(score))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translation of a trained model: and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence : penny is thankful and naomi is alive\n",
      "neural translation : naomi is alive <EOS>\n",
      "reference translation : naomi is alive <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : carlos is kind and paula is uninterested\n",
      "neural translation : paula is uninterested <EOS>\n",
      "reference translation : paula is uninterested <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : jack is hallowed and kent is easy\n",
      "neural translation : kent is easy <EOS>\n",
      "reference translation : kent is easy <EOS>\n",
      "blue score = 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in training_pairs_B[0:3]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = translate(encoderB,\n",
    "                                   decoderB,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(\"neural translation : \" + neural_translation)\n",
    "    reference = t[1] + ' <EOS>'\n",
    "    print(\"reference translation : \" + reference)\n",
    "    reference = reference.split(\" \")\n",
    "    candidate = neural_translation.split(\" \")\n",
    "    score = sentence_bleu([reference], candidate)\n",
    "    print(\"blue score = {:.2f}\".format(score))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting embedding from encoder-decorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedd(encoder,\n",
    "               decoder,\n",
    "               sentence,\n",
    "               max_length=MAX_LENGTH):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        return decoder_hidden.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence : penny is thankful and naomi is alive\n",
      "tensor([-0.9730, -0.8669,  0.9230, -0.1091,  0.5647, -0.4950, -0.9946,  0.8847,\n",
      "         0.9488, -0.7205,  0.1264, -0.8985, -0.7804, -0.7547, -0.2753, -0.6169,\n",
      "        -0.5634,  0.9977,  0.9709,  0.9806,  0.7215, -0.8880,  0.7009, -0.9955,\n",
      "        -0.9997, -0.1647, -0.7316, -0.9343,  0.5175, -0.9337,  0.4206,  0.8430,\n",
      "        -0.5163,  0.1040,  0.1073, -0.9310, -0.9422,  0.0536, -0.6005,  0.0136,\n",
      "        -0.9758, -0.7894, -0.9994, -0.8292, -0.1626, -0.9882,  0.9948,  0.9784,\n",
      "        -0.9217, -0.7311, -0.7362,  0.1502,  0.9211, -0.9356, -0.6400,  0.4647,\n",
      "         0.9651, -0.7759, -0.9999,  0.7200,  0.9999,  0.7536,  0.8866, -0.9582,\n",
      "         0.7626, -0.6714, -0.2845, -0.9990,  0.7893, -0.9921,  0.9604, -0.6677,\n",
      "        -0.7054, -0.9152,  0.9620,  0.8229, -0.7778, -0.8932,  0.9622,  0.9997,\n",
      "        -0.5729, -0.5048,  0.0494, -0.9097, -0.9297,  0.4180, -0.0276,  0.0817,\n",
      "         0.6381, -0.3443,  0.9581, -0.3544, -0.9775, -0.9942, -0.9192, -0.9333,\n",
      "        -0.7522, -0.7951,  0.9657,  0.2204])\n"
     ]
    }
   ],
   "source": [
    "for t in training_pairs_B[0:1]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = get_embedd(encoderB,\n",
    "                                    decoderB,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(neural_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
