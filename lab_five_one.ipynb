{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 5: translation simple ecoder-decocer over the b2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from contra_qa.plots.functions  import simple_step_plot\n",
    "import  matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/boolean2_train.csv\")\n",
    "df2_test = pd.read_csv(\"data/boolean2_test.csv\")\n",
    "\n",
    "df2[\"text\"] = df2[\"sentence1\"] + df2[\"sentence2\"] \n",
    "df2_test[\"text\"] = df2_test[\"sentence1\"] + df2_test[\"sentence2\"] \n",
    "\n",
    "all_sentences = list(df2.text.values) + list(df2_test.text.values)\n",
    "\n",
    "df2train = df2.iloc[:8500]\n",
    "df2valid = df2.iloc[8500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>and_A</th>\n",
       "      <th>and_B</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>Agnes is proud and Lauren is ambitious</td>\n",
       "      <td>Agnes is not proud</td>\n",
       "      <td>Agnes is proud</td>\n",
       "      <td>Lauren is ambitious</td>\n",
       "      <td>1</td>\n",
       "      <td>Agnes is proud and Lauren is ambitious Agnes i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>Curtis is wonderful and Jessie is ambitious</td>\n",
       "      <td>Curtis is not wonderful</td>\n",
       "      <td>Curtis is wonderful</td>\n",
       "      <td>Jessie is ambitious</td>\n",
       "      <td>1</td>\n",
       "      <td>Curtis is wonderful and Jessie is ambitious Cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>Brett is dead and Tracy is important</td>\n",
       "      <td>Tracy is not important</td>\n",
       "      <td>Brett is dead</td>\n",
       "      <td>Tracy is important</td>\n",
       "      <td>1</td>\n",
       "      <td>Brett is dead and Tracy is important Tracy is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>Lauren is hallowed and Yvette is shy</td>\n",
       "      <td>Yvette is not shy</td>\n",
       "      <td>Lauren is hallowed</td>\n",
       "      <td>Yvette is shy</td>\n",
       "      <td>1</td>\n",
       "      <td>Lauren is hallowed and Yvette is shy Yvette is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>Kathleen is alive and Dustin is clever</td>\n",
       "      <td>Dustin is not clever</td>\n",
       "      <td>Kathleen is alive</td>\n",
       "      <td>Dustin is clever</td>\n",
       "      <td>1</td>\n",
       "      <td>Kathleen is alive and Dustin is clever Dustin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentence1                 sentence2  \\\n",
       "8495       Agnes is proud and Lauren is ambitious        Agnes is not proud   \n",
       "8496  Curtis is wonderful and Jessie is ambitious   Curtis is not wonderful   \n",
       "8497         Brett is dead and Tracy is important    Tracy is not important   \n",
       "8498         Lauren is hallowed and Yvette is shy         Yvette is not shy   \n",
       "8499       Kathleen is alive and Dustin is clever      Dustin is not clever   \n",
       "\n",
       "                    and_A                and_B  label  \\\n",
       "8495       Agnes is proud  Lauren is ambitious      1   \n",
       "8496  Curtis is wonderful  Jessie is ambitious      1   \n",
       "8497        Brett is dead   Tracy is important      1   \n",
       "8498   Lauren is hallowed        Yvette is shy      1   \n",
       "8499    Kathleen is alive     Dustin is clever      1   \n",
       "\n",
       "                                                   text  \n",
       "8495  Agnes is proud and Lauren is ambitious Agnes i...  \n",
       "8496  Curtis is wonderful and Jessie is ambitious Cu...  \n",
       "8497  Brett is dead and Tracy is important Tracy is ...  \n",
       "8498  Lauren is hallowed and Yvette is shy Yvette is...  \n",
       "8499  Kathleen is alive and Dustin is clever Dustin ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\n",
      "\n",
      "After: ddddda capoeeeeeee ! ! aas fdf\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "example = \"ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\"\n",
    "print(\"Before:\", example)\n",
    "print()\n",
    "print(\"After:\", normalizeString(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_A = list(zip(list(df2train.sentence1.values), list(df2train.and_A.values)))\n",
    "pairs_B = list(zip(list(df2train.sentence1.values), list(df2train.and_B.values)))\n",
    "pairs_A = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A]\n",
    "pairs_B = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B]\n",
    "pairs_A_val = list(zip(list(df2valid.sentence1.values), list(df2valid.and_A.values)))\n",
    "pairs_B_val = list(zip(list(df2valid.sentence1.values), list(df2valid.and_B.values)))\n",
    "pairs_A_val = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A_val]\n",
    "pairs_B_val = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_pairs = zip(all_sentences, all_sentences)\n",
    "all_text_pairs = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in all_text_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, pairs, reverse=False):\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [tuple(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    cond1 = len(p[0].split(' ')) < MAX_LENGTH\n",
    "    cond2 = len(p[1].split(' ')) < MAX_LENGTH \n",
    "    return cond1 and cond2\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, pairs, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, pairs, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 705\n",
      "eng_dec 704\n",
      "\n",
      "Read 11000 sentence pairs\n",
      "Trimmed to 11000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 706\n",
      "eng_dec 706\n",
      "\n",
      "\n",
      "Read 1500 sentence pairs\n",
      "Trimmed to 1500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 696\n",
      "eng_dec 644\n"
     ]
    }
   ],
   "source": [
    "_, _, training_pairs_A = prepareData(\"eng_enc\",\n",
    "                                             \"eng_dec\",\n",
    "                                             pairs_A)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "input_lang, _, _ = prepareData(\"eng_enc\",\n",
    "                               \"eng_dec\",\n",
    "                               all_text_pairs)\n",
    "\n",
    "output_lang = copy.deepcopy(input_lang)\n",
    "\n",
    "print()\n",
    "\n",
    "print()\n",
    "_, _, valid_pairs_A = prepareData(\"eng_enc\",\n",
    "                                \"eng_dec\",\n",
    "                                pairs_A_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 705\n",
      "eng_dec 704\n",
      "\n",
      "Read 1500 sentence pairs\n",
      "Trimmed to 1500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 696\n",
      "eng_dec 638\n"
     ]
    }
   ],
   "source": [
    "_, _, training_pairs_B = prepareData(\"eng_enc\",\n",
    "                                     \"eng_dec\",\n",
    "                                     pairs_B)\n",
    "print()\n",
    "_, _, valid_pairs_B = prepareData(\"eng_enc\",\n",
    "                                \"eng_dec\",\n",
    "                                pairs_B_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "eng_enc_v_size = input_lang.n_words\n",
    "eng_dec_v_size = output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderA = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoderA = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "encoderA.load_state_dict(torch.load(\"encoder4.pkl\"))\n",
    "decoderA.load_state_dict(torch.load(\"decoder4.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderB = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoderB = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "encoderB.load_state_dict(torch.load(\"encoder5.pkl\"))\n",
    "decoderB.load_state_dict(torch.load(\"decoder5.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder,\n",
    "              decoder,\n",
    "              sentence,\n",
    "              max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translation of a trained model: and A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence : penny is thankful and naomi is alive\n",
      "neural translation : blanche is thankful <EOS>\n",
      "reference translation : penny is thankful <EOS>\n",
      "blue score = 0.71\n",
      "\n",
      "input_sentence : carlos is kind and paula is uninterested\n",
      "neural translation : carlos is kind <EOS>\n",
      "reference translation : carlos is kind <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : jack is hallowed and kent is easy\n",
      "neural translation : jack is hallowed <EOS>\n",
      "reference translation : jack is hallowed <EOS>\n",
      "blue score = 1.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for t in training_pairs_A[0:3]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = translate(encoderA,\n",
    "                                   decoderA,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(\"neural translation : \" + neural_translation)\n",
    "    reference = t[1] + ' <EOS>'\n",
    "    print(\"reference translation : \" + reference)\n",
    "    reference = reference.split(\" \")\n",
    "    candidate = neural_translation.split(\" \")\n",
    "    score = sentence_bleu([reference], candidate)\n",
    "    print(\"blue score = {:.2f}\".format(score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translation of a trained model: and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence : penny is thankful and naomi is alive\n",
      "neural translation : naomi is alive <EOS>\n",
      "reference translation : naomi is alive <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : carlos is kind and paula is uninterested\n",
      "neural translation : paula is uninterested <EOS>\n",
      "reference translation : paula is uninterested <EOS>\n",
      "blue score = 1.00\n",
      "\n",
      "input_sentence : jack is hallowed and kent is easy\n",
      "neural translation : kent is easy <EOS>\n",
      "reference translation : kent is easy <EOS>\n",
      "blue score = 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in training_pairs_B[0:3]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = translate(encoderB,\n",
    "                                   decoderB,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(\"neural translation : \" + neural_translation)\n",
    "    reference = t[1] + ' <EOS>'\n",
    "    print(\"reference translation : \" + reference)\n",
    "    reference = reference.split(\" \")\n",
    "    candidate = neural_translation.split(\" \")\n",
    "    score = sentence_bleu([reference], candidate)\n",
    "    print(\"blue score = {:.2f}\".format(score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AndModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoderA,\n",
    "                 decoderA,\n",
    "                 encoderB,\n",
    "                 decoderB,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 max_length,\n",
    "                 input_lang,\n",
    "                 target_lang,\n",
    "                 SOS_token=0,\n",
    "                 EOS_token=1):\n",
    "        super(AndModel, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.encoderA = encoderA\n",
    "        self.decoderA = decoderA\n",
    "        self.encoderB = encoderB\n",
    "        self.decoderB = decoderB\n",
    "        self.input_lang = input_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.SOS_token = SOS_token\n",
    "        self.EOS_token = EOS_token\n",
    "        \n",
    "    \n",
    "    def encode(self,\n",
    "               sentence,\n",
    "               encoder,\n",
    "               is_tensor,\n",
    "               hidden=None):\n",
    "        if not is_tensor:\n",
    "            input_tensor = tensorFromSentence(self.input_lang, sentence)\n",
    "        else:\n",
    "            input_tensor = sentence\n",
    "\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        if hidden is None:\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "        else:\n",
    "            encoder_hidden = hidden\n",
    "            \n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            _, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                        encoder_hidden)\n",
    "        return encoder_hidden\n",
    "    \n",
    "    \n",
    "    def decode(self,\n",
    "               tensor,\n",
    "               decoder,\n",
    "               out_tensor):\n",
    "        \n",
    "        decoder_input = torch.tensor([[self.SOS_token]], device=device)\n",
    "        decoder_hidden = tensor\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(self.max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == self.EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(self.target_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "        \n",
    "        if not out_tensor:\n",
    "            output = \" \".join(decoded_words)\n",
    "        else:\n",
    "            output = decoder_hidden\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def sen2vec(self, sentence, encoder, decoder, is_tensor, out_tensor):\n",
    "        encoded = self.encode(sentence, encoder, is_tensor)\n",
    "        vec = self.decode(encoded, decoder, out_tensor)\n",
    "        return vec\n",
    "    \n",
    "    def sen2vecA(self, sentence, is_tensor):\n",
    "        encoded = self.encode(sentence, self.encoderA, is_tensor)\n",
    "        vec = self.decode(encoded, self.decoderA, out_tensor=True)\n",
    "        return vec\n",
    "    \n",
    "    def sen2vecB(self, sentence, is_tensor):\n",
    "        encoded = self.encode(sentence, self.encoderB, is_tensor)\n",
    "        vec = self.decode(encoded, self.decoderB, out_tensor=True)\n",
    "        return vec\n",
    "    \n",
    "    def forward(self, s1, s2, is_tensor=True):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AndModel(encoderA,\n",
    "             decoderA,\n",
    "             encoderB,\n",
    "             decoderB,\n",
    "             hidden_size=100,\n",
    "             output_size=2,\n",
    "             max_length=MAX_LENGTH,\n",
    "             input_lang=input_lang,\n",
    "             target_lang=output_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test encoding decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "s1:\n",
      "\n",
      "penny is thankful and naomi is alive\n",
      "\n",
      "inference A:\n",
      "\n",
      "blanche is thankful <EOS>\n",
      "\n",
      "inference B:\n",
      "\n",
      "naomi is alive <EOS>\n",
      "===========\n",
      "s1:\n",
      "\n",
      "carlos is kind and paula is uninterested\n",
      "\n",
      "inference A:\n",
      "\n",
      "carlos is kind <EOS>\n",
      "\n",
      "inference B:\n",
      "\n",
      "paula is uninterested <EOS>\n",
      "===========\n",
      "s1:\n",
      "\n",
      "jack is hallowed and kent is easy\n",
      "\n",
      "inference A:\n",
      "\n",
      "jack is hallowed <EOS>\n",
      "\n",
      "inference B:\n",
      "\n",
      "kent is easy <EOS>\n"
     ]
    }
   ],
   "source": [
    "for ex in training_pairs_B[0:3]:\n",
    "    print(\"===========\")\n",
    "    ex = ex[0]\n",
    "    print(\"s1:\\n\")\n",
    "    print(ex)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    ex_A = a.sen2vec(ex,\n",
    "                     a.encoderA,\n",
    "                     a.decoderA,\n",
    "                     is_tensor=False,\n",
    "                     out_tensor=False)\n",
    "    \n",
    "    ex_B = a.sen2vec(ex,\n",
    "                     a.encoderB,\n",
    "                     a.decoderB,\n",
    "                     is_tensor=False,\n",
    "                     out_tensor=False)\n",
    "\n",
    "    print(\"inference A:\\n\")\n",
    "    print(ex_A)\n",
    "    print()\n",
    "    print(\"inference B:\\n\")\n",
    "    print(ex_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "s1:\n",
      "\n",
      "penny is thankful and naomi is alive\n",
      "\n",
      "tensor([[[-0.4259, -0.5321,  0.6835,  0.9980,  0.9865, -0.0326, -0.8547,\n",
      "          -0.6571,  0.5157,  0.0794,  0.4141, -0.3890,  1.0000, -0.9051,\n",
      "           0.9944,  0.9096,  0.0875, -0.9996,  0.9112, -0.0006, -0.9984,\n",
      "          -0.7040, -0.0880, -0.7222, -0.9990,  0.7732, -0.9064,  0.2055,\n",
      "           0.1898,  0.8505, -0.6891, -0.9391,  0.9999, -0.9916,  0.5694,\n",
      "           0.3941,  0.8801,  0.9980, -0.5498, -0.8040, -0.1095, -0.2328,\n",
      "          -0.9724,  0.8991, -0.2609, -0.0054,  0.9327, -0.9945, -0.1388,\n",
      "          -0.9704, -0.9887,  0.9991,  0.8406,  0.0321, -0.9388, -0.6922,\n",
      "           0.0622, -0.9972, -0.9954,  0.8046, -0.4433,  0.9741, -0.8358,\n",
      "          -0.9171, -0.9966, -0.4546,  0.9799, -0.1956,  0.9755, -0.5717,\n",
      "          -0.6437, -0.4731, -0.7418,  0.9960, -0.9396,  0.0081,  0.4872,\n",
      "          -0.9808, -0.2468,  0.7522,  0.2154,  0.9563,  0.9638,  0.5954,\n",
      "           0.8221, -0.0410,  0.8202, -0.9973,  0.4164, -0.1902, -0.9980,\n",
      "           0.9997,  0.1830, -0.9278, -0.6124, -0.0751, -0.8409, -0.3582,\n",
      "          -0.8715,  0.9622]]], grad_fn=<ViewBackward>)\n",
      "\n",
      "tensor([[[ 0.9681,  0.9549, -0.9836, -0.8096,  0.7656,  0.7503,  0.4810,\n",
      "          -0.9997,  0.3185,  0.9945, -0.6052,  0.8041, -0.9163,  0.8263,\n",
      "           0.9217, -0.9635, -1.0000, -0.9990,  0.9566,  0.9749,  0.8599,\n",
      "           0.8530,  0.9384, -0.4741, -0.9522, -0.0343, -0.4454,  0.7290,\n",
      "          -0.8507,  0.9431,  0.9202, -0.9189,  0.7365,  0.1136, -0.7705,\n",
      "          -0.1490,  0.9824, -0.9364,  0.9904,  0.2618, -0.3374, -0.9118,\n",
      "           0.1381,  0.9878, -0.6004, -0.5643, -0.9887,  0.9235,  0.9662,\n",
      "          -0.7987, -0.9980,  0.9948, -0.9022,  0.1021, -0.0479, -0.5441,\n",
      "           0.9980,  0.9634,  0.9104, -0.8844,  0.6518, -0.2642, -0.6093,\n",
      "          -0.9827, -0.2343,  0.9369, -0.8936, -0.9938,  0.7012, -0.2052,\n",
      "          -0.9611,  0.9596,  0.9925, -0.9656, -0.9996,  0.6617,  0.9981,\n",
      "           0.9454, -0.9839,  0.6668,  0.9806,  0.2761,  0.7207,  0.8011,\n",
      "          -0.1732,  0.9932, -0.7831,  0.9244,  0.0141, -0.2109,  0.9562,\n",
      "          -0.9118,  0.6554,  0.4418, -0.0870, -0.9862, -0.5640,  0.9495,\n",
      "           0.9989,  0.9004]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "for ex in training_pairs_B[0:1]:\n",
    "    print(\"===========\")\n",
    "    ex = ex[0]\n",
    "    print(\"s1:\\n\")\n",
    "    print(ex)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    ex_A = a.sen2vec(ex,\n",
    "                     a.encoderA,\n",
    "                     a.decoderA,\n",
    "                     is_tensor=False,\n",
    "                     out_tensor=False)\n",
    "    ex_A = a.sen2vecA(ex,is_tensor=False)\n",
    "    ex_B = a.sen2vecB(ex,is_tensor=False)\n",
    "    \n",
    "    print(ex_A)\n",
    "    print()\n",
    "    print(ex_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
