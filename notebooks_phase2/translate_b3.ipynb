{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation simple ecoder-decocer over the b3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from contra_qa.plots.functions  import simple_step_plot\n",
    "import  matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/boolean3_train.csv\")\n",
    "df2_test = pd.read_csv(\"data/boolean3_test.csv\")\n",
    "\n",
    "\n",
    "df2[\"text\"] = df2[\"sentence1\"] + df2[\"sentence2\"] \n",
    "df2_test[\"text\"] = df2_test[\"sentence1\"] + df2_test[\"sentence2\"] \n",
    "\n",
    "all_sentences = list(df2.text.values) + list(df2_test.text.values)\n",
    "\n",
    "df2train = df2.iloc[:8500]\n",
    "df2valid = df2.iloc[8500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>and_A</th>\n",
       "      <th>and_B</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>Milton went to Timisoara and Tulcea</td>\n",
       "      <td>Shawn didn't go to Tulcea</td>\n",
       "      <td>Milton went to Timisoara</td>\n",
       "      <td>Milton went to Tulcea</td>\n",
       "      <td>0</td>\n",
       "      <td>Milton went to Timisoara and TulceaShawn didn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>Alice has traveled to Giurgiu and Craiova</td>\n",
       "      <td>Alice didn't travel to Craiova</td>\n",
       "      <td>Alice has traveled to Giurgiu</td>\n",
       "      <td>Alice has traveled to Craiova</td>\n",
       "      <td>1</td>\n",
       "      <td>Alice has traveled to Giurgiu and CraiovaAlice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>Blanche went to Slatina and Cluj-Napoca</td>\n",
       "      <td>Blanche didn't go to Bucharest</td>\n",
       "      <td>Blanche went to Slatina</td>\n",
       "      <td>Blanche went to Cluj-Napoca</td>\n",
       "      <td>0</td>\n",
       "      <td>Blanche went to Slatina and Cluj-NapocaBlanche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>Nicole went to Reghin and Blaj</td>\n",
       "      <td>Nicole didn't go to Reghin</td>\n",
       "      <td>Nicole went to Reghin</td>\n",
       "      <td>Nicole went to Blaj</td>\n",
       "      <td>1</td>\n",
       "      <td>Nicole went to Reghin and BlajNicole didn't go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>Sheryl went to Cluj-Napoca and Hunedoara</td>\n",
       "      <td>Sheryl didn't go to Hunedoara</td>\n",
       "      <td>Sheryl went to Cluj-Napoca</td>\n",
       "      <td>Sheryl went to Hunedoara</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheryl went to Cluj-Napoca and HunedoaraSheryl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentence1  \\\n",
       "8495        Milton went to Timisoara and Tulcea   \n",
       "8496  Alice has traveled to Giurgiu and Craiova   \n",
       "8497    Blanche went to Slatina and Cluj-Napoca   \n",
       "8498             Nicole went to Reghin and Blaj   \n",
       "8499   Sheryl went to Cluj-Napoca and Hunedoara   \n",
       "\n",
       "                           sentence2                          and_A  \\\n",
       "8495       Shawn didn't go to Tulcea       Milton went to Timisoara   \n",
       "8496  Alice didn't travel to Craiova  Alice has traveled to Giurgiu   \n",
       "8497  Blanche didn't go to Bucharest        Blanche went to Slatina   \n",
       "8498      Nicole didn't go to Reghin          Nicole went to Reghin   \n",
       "8499   Sheryl didn't go to Hunedoara     Sheryl went to Cluj-Napoca   \n",
       "\n",
       "                              and_B  label  \\\n",
       "8495          Milton went to Tulcea      0   \n",
       "8496  Alice has traveled to Craiova      1   \n",
       "8497    Blanche went to Cluj-Napoca      0   \n",
       "8498            Nicole went to Blaj      1   \n",
       "8499       Sheryl went to Hunedoara      1   \n",
       "\n",
       "                                                   text  \n",
       "8495  Milton went to Timisoara and TulceaShawn didn'...  \n",
       "8496  Alice has traveled to Giurgiu and CraiovaAlice...  \n",
       "8497  Blanche went to Slatina and Cluj-NapocaBlanche...  \n",
       "8498  Nicole went to Reghin and BlajNicole didn't go...  \n",
       "8499  Sheryl went to Cluj-Napoca and HunedoaraSheryl...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\n",
      "\n",
      "After: ddddda capoeeeeeee ! ! aas fdf\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "example = \"ddddda'''~~çãpoeéééééÈ'''#$$##@!@!@AAS@#12323fdf\"\n",
    "print(\"Before:\", example)\n",
    "print()\n",
    "print(\"After:\", normalizeString(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_A = list(zip(list(df2train.sentence1.values), list(df2train.and_A.values)))\n",
    "pairs_B = list(zip(list(df2train.sentence1.values), list(df2train.and_B.values)))\n",
    "pairs_A = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A]\n",
    "pairs_B = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B]\n",
    "pairs_A_val = list(zip(list(df2valid.sentence1.values), list(df2valid.and_A.values)))\n",
    "pairs_B_val = list(zip(list(df2valid.sentence1.values), list(df2valid.and_B.values)))\n",
    "pairs_A_val = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_A_val]\n",
    "pairs_B_val = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in pairs_B_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_pairs = zip(all_sentences, all_sentences)\n",
    "all_text_pairs = [(normalizeString(s1), normalizeString(s2)) for s1, s2 in all_text_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, pairs, reverse=False):\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [tuple(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    cond1 = len(p[0].split(' ')) < MAX_LENGTH\n",
    "    cond2 = len(p[1].split(' ')) < MAX_LENGTH \n",
    "    return cond1 and cond2\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, pairs, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, pairs, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 685\n",
      "eng_dec 683\n",
      "\n",
      "Read 11000 sentence pairs\n",
      "Trimmed to 11000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 8149\n",
      "eng_dec 8149\n",
      "\n",
      "Read 1500 sentence pairs\n",
      "Trimmed to 1500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 652\n",
      "eng_dec 612\n"
     ]
    }
   ],
   "source": [
    "_, _, training_pairs_A = prepareData(\"eng_enc\",\n",
    "                                     \"eng_dec\",\n",
    "                                     pairs_A)\n",
    "print()\n",
    "\n",
    "\n",
    "input_lang, _, _ = prepareData(\"eng_enc\",\n",
    "                               \"eng_dec\",\n",
    "                               all_text_pairs)\n",
    "\n",
    "output_lang = copy.deepcopy(input_lang)\n",
    "\n",
    "print()\n",
    "\n",
    "_, _, valid_pairs_A = prepareData(\"eng_enc\",\n",
    "                                \"eng_dec\",\n",
    "                                pairs_A_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8500 sentence pairs\n",
      "Trimmed to 8500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 685\n",
      "eng_dec 683\n",
      "\n",
      "Read 1500 sentence pairs\n",
      "Trimmed to 1500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng_enc 652\n",
      "eng_dec 608\n"
     ]
    }
   ],
   "source": [
    "_, _, training_pairs_B = prepareData(\"eng_enc\",\n",
    "                                     \"eng_dec\",\n",
    "                                     pairs_B)\n",
    "print()\n",
    "_, _, valid_pairs_B = prepareData(\"eng_enc\",\n",
    "                                \"eng_dec\",\n",
    "                                pairs_B_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentences 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('judy has traveled to turda and vaslui', 'judy has traveled to turda')\n"
     ]
    }
   ],
   "source": [
    "example = random.choice(training_pairs_A)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[132, 14, 6, 7, 18, 3, 53]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFromSentence(input_lang,example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[132, 14, 6, 7, 18]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFromSentence(output_lang, example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[132],\n",
      "        [ 14],\n",
      "        [  6],\n",
      "        [  7],\n",
      "        [ 18],\n",
      "        [  3],\n",
      "        [ 53],\n",
      "        [  1]])\n",
      "\n",
      "torch.Size([8, 1])\n",
      "torch.int64\n",
      "tensor([[132],\n",
      "        [ 14],\n",
      "        [  6],\n",
      "        [  7],\n",
      "        [ 18],\n",
      "        [  1]])\n",
      "\n",
      "torch.Size([6, 1])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_sen = tensorFromSentence(input_lang,example[0])\n",
    "output_sen = tensorFromSentence(output_lang, example[1])\n",
    "\n",
    "print(input_sen)\n",
    "print()\n",
    "print(input_sen.shape)\n",
    "print(input_sen.dtype)\n",
    "print(output_sen)\n",
    "print()\n",
    "print(output_sen.shape)\n",
    "print(output_sen.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "\n",
      "tensor([[132],\n",
      "        [ 14],\n",
      "        [  6],\n",
      "        [  7],\n",
      "        [ 18],\n",
      "        [  3],\n",
      "        [ 53],\n",
      "        [  1]])\n",
      "\n",
      "torch.Size([8, 1])\n",
      "torch.int64\n",
      "\n",
      "output\n",
      "\n",
      "tensor([[132],\n",
      "        [ 14],\n",
      "        [  6],\n",
      "        [  7],\n",
      "        [ 18],\n",
      "        [  1]])\n",
      "\n",
      "torch.Size([6, 1])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_sen, output_sen = tensorsFromPair(example)\n",
    "\n",
    "\n",
    "print(\"input\\n\")\n",
    "print(input_sen)\n",
    "print()\n",
    "print(input_sen.shape)\n",
    "print(input_sen.dtype)\n",
    "\n",
    "print(\"\\noutput\\n\")\n",
    "print(output_sen)\n",
    "print()\n",
    "print(output_sen.shape)\n",
    "print(output_sen.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "eng_enc_v_size = input_lang.n_words\n",
    "eng_dec_v_size = output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sen: torch.Size([8, 1]) torch.int64\n",
      "h0: torch.Size([1, 1, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "h0 = encoder.initHidden()\n",
    "print(\"input_sen:\", input_sen.shape, input_sen.dtype)\n",
    "print(\"h0:\", h0.shape, h0.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([1, 1, 10]) torch.float32\n",
      "hidden_enc: torch.Size([1, 1, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "max_length = MAX_LENGTH\n",
    "encoder_outputs = torch.zeros(max_length,\n",
    "                              encoder.hidden_size,\n",
    "                              device=device)\n",
    "\n",
    "input_length = input_sen.size(0)\n",
    "\n",
    "for ei in range(input_length):\n",
    "    output, hidden_enc = encoder(input_sen[ei], h0)\n",
    "    h0 = hidden_enc\n",
    "    encoder_outputs[ei] = output[0, 0]\n",
    "\n",
    "print(\"output:\", output.shape, output.dtype)\n",
    "print(\"hidden_enc:\", hidden_enc.shape, hidden_enc.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output: torch.Size([1, 8149]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 8149]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 8149]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 8149]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 8149]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n",
      "decoder_output: torch.Size([1, 8149]) torch.float32\n",
      "\n",
      "decoder_hidden: torch.Size([1, 1, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "decoder_hidden = hidden_enc\n",
    "\n",
    "target_length = output_sen.size(0)\n",
    "\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    decoder_input = output_sen[di]  # Teacher forcing\n",
    "    print(\"decoder_output:\", decoder_output.shape, decoder_output.dtype)\n",
    "    print()\n",
    "    print(\"decoder_hidden:\", decoder_hidden.shape, decoder_hidden.dtype)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss over each token of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor,\n",
    "          target_tensor,\n",
    "          encoder,\n",
    "          decoder,\n",
    "          encoder_optimizer,\n",
    "          decoder_optimizer,\n",
    "          criterion,\n",
    "          max_length,\n",
    "          teacher_forcing_ratio=0.5):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length,\n",
    "                                  encoder.hidden_size,\n",
    "                                  device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True\n",
    "\n",
    "    if not random.random() < teacher_forcing_ratio:\n",
    "        use_teacher_forcing = False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.topk(1)\n",
    "            decoder_input = topone.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(input_tensor,\n",
    "             target_tensor,\n",
    "             encoder,\n",
    "             decoder,\n",
    "             criterion,\n",
    "             max_length):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length,\n",
    "                                  encoder.hidden_size,\n",
    "                                  device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        _, topone = decoder_output.topk(1)\n",
    "        decoder_input = topone.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test get loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss 9.064399078647673\n"
     ]
    }
   ],
   "source": [
    "valid_pairs = [tensorsFromPair(pair) for pair in valid_pairs_A]\n",
    "valid_loss = []\n",
    "for t in valid_pairs:\n",
    "    input_sen, output_sen = t\n",
    "    loss = get_loss(input_sen,\n",
    "                    output_sen,\n",
    "                    encoder,\n",
    "                    decoder,\n",
    "                    criterion,\n",
    "                    MAX_LENGTH)\n",
    "    valid_loss.append(loss)\n",
    "print(\"mean loss\", np.mean(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    return '%s' % asMinutes(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5bb2630d55d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                  \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                  \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                  max_length=MAX_LENGTH)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-c0c11abc6b26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iters = 1000\n",
    "training_pairs_little = [tensorsFromPair(random.choice(training_pairs_A)) for i in range(n_iters)]\n",
    "\n",
    "losses = []\n",
    "start = time.time()\n",
    "\n",
    "for t in training_pairs_little:\n",
    "    input_sen, output_sen = t    \n",
    "    loss = train(input_sen,\n",
    "                 output_sen,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 encoder_optimizer,\n",
    "                 decoder_optimizer,\n",
    "                 criterion,\n",
    "                 max_length=MAX_LENGTH)\n",
    "    losses.append(loss)\n",
    "\n",
    "print(timeSince(start))\n",
    "\n",
    "simple_step_plot([losses],\n",
    "                 \"loss\",\n",
    "                 \"loss example ({} pair of sentences only)\".format(n_iters),\n",
    "                 \"loss_example.png\",\n",
    "                  figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder,\n",
    "               decoder,\n",
    "               n_iters,\n",
    "               pairs,\n",
    "               valid_pairs,\n",
    "               encoder_path,\n",
    "               decoder_path,\n",
    "               batch_size=32,\n",
    "               status_every=100,\n",
    "               learning_rate=0.01,\n",
    "               teacher_forcing_ratio=0.5):\n",
    "\n",
    "    plot_losses = []\n",
    "    old = 0\n",
    "    start = time.time()\n",
    "    all_loss = []\n",
    "    valid_loss = float(\"inf\")\n",
    "    \n",
    "    \n",
    "\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "    for i, t in enumerate(training_pairs):\n",
    "        input_sen, output_sen = t\n",
    "        loss = train(input_sen,\n",
    "                     output_sen,\n",
    "                     encoder,\n",
    "                     decoder,\n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer,\n",
    "                     criterion,\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        plot_losses.append(loss)\n",
    "\n",
    "        if i % status_every == 0 and i != 0:\n",
    "            valid_batch = [tensorsFromPair(random.choice(valid_pairs))\n",
    "                           for i in range(batch_size)]\n",
    "            batch_loss = 0\n",
    "            for t in valid_batch:\n",
    "                input_sen, output_sen = t\n",
    "                batch_loss += get_loss(input_sen,\n",
    "                                       output_sen,\n",
    "                                       encoder,\n",
    "                                       decoder,\n",
    "                                       criterion,\n",
    "                                       MAX_LENGTH)\n",
    "            current_valid_loss = batch_loss / batch_size\n",
    "            \n",
    "            if current_valid_loss < valid_loss:\n",
    "                valid_loss = current_valid_loss\n",
    "                torch.save(encoder.state_dict(), encoder_path)\n",
    "                torch.save(decoder.state_dict(), decoder_path)\n",
    "            print(\"mean training loss = {:.2f}\".format(np.mean(plot_losses)))\n",
    "            print(\"mean valid loss = {:.2f}\".format(current_valid_loss))\n",
    "            print(\"time in {} steps:\".format(status_every), timeSince(start))\n",
    "            print()\n",
    "#             simple_step_plot([plot_losses],\n",
    "#                              \"loss\",\n",
    "#                              \"loss plot (from {} to {})\".format(old, i),\n",
    "#                              \"loss_example.png\",\n",
    "#                              figsize=(10, 3))\n",
    "            all_loss += plot_losses\n",
    "            plot_losses = []\n",
    "            old = i\n",
    "            start = time.time()\n",
    "    \n",
    "    simple_step_plot([all_loss],\n",
    "                     \"loss\",\n",
    "                     \"loss over training\" ,\n",
    "                     \"loss_example.png\",\n",
    "                     figsize=(15, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder,\n",
    "              decoder,\n",
    "              sentence,\n",
    "              max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, topone = decoder_output.data.topk(1)\n",
    "            if topone.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topone.item()])\n",
    "\n",
    "            decoder_input = topone.squeeze().detach()\n",
    "\n",
    "        return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translation of a non trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "np.random.shuffle(training_pairs_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in training_pairs_A[0:3]:\n",
    "    print(\"input_sentence : \" + t[0])\n",
    "    neural_translation = translate(encoder,\n",
    "                                   decoder,\n",
    "                                   t[0],\n",
    "                                   max_length=MAX_LENGTH)\n",
    "    print(\"neural translation : \" + neural_translation)\n",
    "    reference = t[1] + ' <EOS>'\n",
    "    print(\"reference translation : \" + reference)\n",
    "    reference = reference.split(\" \")\n",
    "    candidate = neural_translation.split(\" \")\n",
    "    score = sentence_bleu([reference], candidate)\n",
    "    print(\"blue score = {:.2f}\".format(score))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training some models and observing its translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_translation(pairs, encoder, decoder, max_length, out_path):\n",
    "    with open(out_path, \"w\") as file:\n",
    "        file.write(\"source,candidate,reference,blue,accuracy\\n\")        \n",
    "        for tuple_ in pairs:\n",
    "            source, reference = tuple_\n",
    "            candidate = translate(encoder,\n",
    "                                  decoder,\n",
    "                                  source,\n",
    "                                  max_length=max_length)\n",
    "            reference =  reference + ' <EOS>'\n",
    "            blue = sentence_bleu([reference.split(\" \")], candidate.split(\" \"))\n",
    "            if blue >= 0.95:\n",
    "                acc = 1\n",
    "            else:\n",
    "                acc = 0\n",
    "            line = source + \",\"\n",
    "            line += candidate + \",\"\n",
    "            line += reference + \",\"\n",
    "            line += \"{:.3f},\".format(blue)\n",
    "            line += \"{}\\n\".format(acc)\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test save_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation(training_pairs_A[0:3],\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"temp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "\n",
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "trainIters(encoder=encoder,\n",
    "           decoder=decoder,\n",
    "           n_iters=5000,\n",
    "           pairs=training_pairs_A,\n",
    "           valid_pairs=valid_pairs_A,\n",
    "           encoder_path=\"b3_encoder1.pkl\",\n",
    "           decoder_path=\"b3_decoder1.pkl\",\n",
    "           status_every=200,\n",
    "           learning_rate=0.02,\n",
    "           teacher_forcing_ratio=0.2)\n",
    "\n",
    "\n",
    "save_translation(training_pairs_A,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"b3_training1.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"b3_training1.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over training data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over training data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "\n",
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "trainIters(encoder=encoder,\n",
    "           decoder=decoder,\n",
    "           n_iters=5000,\n",
    "           pairs=training_pairs_B,\n",
    "           valid_pairs=valid_pairs_B,\n",
    "           encoder_path=\"b3_encoder2.pkl\",\n",
    "           decoder_path=\"b3_decoder2.pkl\",\n",
    "           status_every=200,\n",
    "           learning_rate=0.02,\n",
    "           teacher_forcing_ratio=0.5)\n",
    "\n",
    "\n",
    "save_translation(training_pairs_A,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"b3_training2.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"b3_training2.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over training data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over training data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "\n",
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"b3_encoder1.pkl\"))\n",
    "decoder.load_state_dict(torch.load(\"b3_decoder1.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation(training_pairs_A,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"b3_training1.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"b3_training1.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over training data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over training data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation(valid_pairs_A,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"b3_valid1.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"b3_valid1.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over valid data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over valid data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "\n",
    "encoder = EncoderRNN(eng_enc_v_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, eng_dec_v_size)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"b3_encoder2.pkl\"))\n",
    "decoder.load_state_dict(torch.load(\"b3_decoder2.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation(training_pairs_B,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"b3_training2.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"b3_training2.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over training data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over training data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation(valid_pairs_B,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 MAX_LENGTH,\n",
    "                 \"b3_valid2.csv\")\n",
    "\n",
    "df_results = pd.read_csv(\"b3_valid2.csv\")\n",
    "acc = np.mean(df_results.accuracy.values)\n",
    "blue = np.mean(df_results.blue.values)\n",
    "print(\"mean blue score over valid data = {:.3f}\".format(blue))\n",
    "print(\"mean acc over valid data = {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
